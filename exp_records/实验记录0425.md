
## 实验分析

### 问题1 训练不出效果
- 问题描述
之前用TD3在稠密奖励的jsbsim上训练不出策略。

- 解决方案
利用标准化后的特征，将网络生成的动作映射到指定区间。

### 问题2 稠密奖励环境中训练出来的策略到不了目标点

- 两方面的改进思路
调整td3的超参数
修改环境的奖励，降低过程中得到的奖励

- 生效方案
通过放宽到终点的距离误差，来缓解超时的问题。

### 问题3 稀疏环境中，用奖励重标记训练，出现性能骤降

- 问题描述
loss没有问题，突然一下子，性能逐渐降下来了
是否可以理解为，当策略保持的时候，使其收集的样本趋于同质化，不利于网络更新。
解决办法就是增大buffer size，或者减缓网络更新，或者正则化。

- 生效方案
增大buffer size，同时进行学习率的衰减